# -*- coding: utf-8 -*-
"""MinGPT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12rn5HuB0VK7msL5chUBAEhyyFUxzkN46

Getting the Dataset
"""

!wget https://github.com/Fariaz99cuet/Character_level_GPT/blob/main/lotr.txt

import torch

"""Config  Parameters

"""

max_iters=10000
eval_iters=200
learning_rate = 1e-3
batch_size = 16 # how many independent sequences will we process in parallel?
block_size = 32
device = 'cuda' if torch.cuda.is_available() else 'cpu'
eval_iters = 200
n_embd = 64
n_head = 4
n_layer = 4
dropout = 0.0
eval_interval = 100

from google.colab import drive
drive.mount('/content/drive')

"""Lord of the rings.txt

```


"""

with open('lotr.txt.1','r', encoding ='utf-8') as f:

  input =f.read()

print("Length of dataset:",len(input))

#looking at first 500 characters
print(input[:500])

"""Vocab_size and vocabolary"""

chars = sorted(list(set(input)))
vocab_size = len(chars)
print(''.join(chars))
print(vocab_size)

print(vocab_size)

"""Character Level simple Tokenization

"""

#creating a character to integer mapping
stoi = {ch:i for i,ch in enumerate(chars) }
itos={i:ch for i,ch in enumerate(chars)}
encode = lambda e : [stoi[c] for c in e]
decode = lambda l: ''.join([itos[i] for i in l])

print(encode("When you play the game of thrones, you win or you die"))
print(decode(encode("When you play the game of thrones, you win or you die")))

#encoding the entire text and storing it in torch tensor

import torch
data = torch.tensor(encode(input), dtype=torch.long)

print(data.shape, data.dtype)
print(data[:1000])

block_size =8
batch_size=4
ix = torch.randint(len(data) - block_size, (batch_size,))
ix

x = torch.stack([data[i:i+block_size] for i in ix])
 x

#spliting the data into train and validation set

n =int(0.9*len(data))

train = data[:n]

val = data[n:]

block_size = 8
train[:block_size+1]

x =train[:block_size]
y = train[1:block_size+1]
print(x,y)

"""Getting Batch Data"""

def get_batch(split):
  data =train if split=="train" else val
  ix=torch.randint(len(data)-block_size,(batch_size,))
  x=torch.stack([data[i:i+block_size] for i in ix])
  y=torch.stack([data[i+1:i+block_size+1] for i in ix])
  x,y=x.to(device),y.to(device)
  return x,y

"""Loss Estimation"""

eval_iters=200
@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(eval_iters)
        for k in range(eval_iters):
            X, Y = get_batch(split)
            logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out

import torch.nn as nn
from torch.nn import functional as F
n_embed=128

"""Single Headed Attention"""

class Head(nn.Module):

  """One headed attention"""

  def __init__(self,head_size):
    super().__init__()
    self.key=nn.Linear(n_embed,head_size,bias=False)
    self.Query=nn.Linear(n_embed,head_size,bias=False)
    self.Value=nn.Linear(n_embed,head_size,bias=False)
    self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))

    self.dropout =nn.Dropout(dropout)

  def forward(self,x):
    B,T,C = x.shape
    k = self.key(x) #(B,T,C)
    q = self.Query(x) #(B,T,C)

    wei= q @ k.transpose(-2,-1) ##(B,T,C) @ (B,C,T) = (B,T,T)
    wei= wei.masked_fill(self.tril[:T,:T] ==0,float('-inf')) #(B,T,T)
    wei =F.softmax(wei,dim=-1) #(B,T,T)
    wei=self.dropout(wei)
    v=self.Value(x)#(B,T,C)

    out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)

    return out

"""Multi Head Attention"""

class MultiHeadAttention(nn.Module):
  "Multi headed attention "
  def __init__(self,num_head,head_size):
    super().__init__()
    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_head)])
    self.proj = nn.Linear(n_embed,n_embed)
    self.dropout = nn.Dropout(dropout)

  def forward(self,x):

    out = torch.cat([h(x) for h in self.heads],dim=-1)

    out = self.dropout(self.proj(out))

    return out

"""Feed forward network"""

class FeedForward(nn.Module):

  def __init__(self,n_embed):
    super().__init__()
    self.net =nn.Sequential(
        nn.Linear(n_embed,4*n_embed),
        nn.ReLU(),
        nn.Linear(4*n_embed,n_embed),
        nn.Dropout(dropout),)

  def forward(self,x):
    return self.net(x)

"""Single Transformer Block"""

class Block(nn.Module):

  def __init__(self, n_embd, n_head):
    super().__init__()
    head_size=n_embed//n_head
    self.sa=MultiHeadAttention(n_head,head_size)
    self.ffwd=FeedForward(n_embed)
    self.ln1=nn.LayerNorm(n_embed)
    self.ln2=nn.LayerNorm(n_embed)

  def forward(self,x):
    x = x + self.sa(self.ln1(x))
    x = x + self.ffwd(self.ln2(x))
    return x



"""Bigram Model for Text Generation"""

#Bigram model

n_layers =16

class BigramLanguageModel(nn.Module):

  def __init__(self):

     super().__init__()
     #each token reads logits of next token from embedding lookup table

     self.token_embedding_table = nn.Embedding(vocab_size,n_embed)
     self.position_embedding_table=nn.Embedding(block_size,n_embed)
     self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])
     self.ln=nn.LayerNorm(n_embed)
     self.ln_head=nn.Linear(n_embed,vocab_size)

  def forward(self,idx,target=None):
    B,T = idx.shape

    tok_emb = self.token_embedding_table(idx)
    pos_emb = self.position_embedding_table(torch.arange(T,device=device))
    x = tok_emb + pos_emb #(B,T,c)
    x = self.blocks(x)#(B,T,c)
    x = self.ln(x)#(B,T,c)
    logits = self.ln_head(x) #(B,T,vocab_size)

    if target is None:
      loss=None
    else :
      B, T, C = logits.shape

      logits = logits.view(B*T,C)
      target = target.view(B*T)

      loss=F.cross_entropy(logits,target)

    return logits,loss


  def generate(self,idx,max_new_tokens):



    for _ in range(max_new_tokens):

      idx_cond = idx[:,-block_size:]

      logits,loss=self(idx_cond)

      logits = logits[:,-1,:] #(B,C)

      prob = F.softmax(logits,dim=-1)

      idx_next = torch.multinomial(prob,num_samples=1)

      idx=torch.cat((idx,idx_next),dim=1)

    return idx

"""Training the model"""

model = BigramLanguageModel()

m=model.to(device)

print(sum(p.numel() for p in model.parameters())/1e6,'M Parameters')

optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)

for iter in range(max_iters):

  if iter % eval_interval==0 or iter==eval_interval-1:

    losses=estimate_loss()
    print(f"step {iter} : train_loss={losses['train']:.4f} val_loss={losses['val']:.4f}")

    x_batch,y_batch=get_batch('train')

    logits,loss=model(x_batch,y_batch)

    optimizer.zero_grad(set_to_none=None)
    loss.backward()
    optimizer.step()

"""Generation Of text"""

context = torch.zeros((1, 1), dtype=torch.long, device=device)
print(decode(m.generate(context, max_new_tokens=1500)[0].tolist()))