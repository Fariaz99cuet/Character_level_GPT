{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Getting the Dataset\n"
      ],
      "metadata": {
        "id": "awGg0pWXn6Ag"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGSqdFYyRIto",
        "outputId": "4d35f80b-d415-470c-fe63-16ee2d19e0ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-14 10:57:33--  https://github.com/Fariaz99cuet/Character_level_GPT/blob/main/lotr.txt\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘lotr.txt.1’\n",
            "\n",
            "\rlotr.txt.1              [<=>                 ]       0  --.-KB/s               \rlotr.txt.1              [ <=>                ] 143.97K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2024-04-14 10:57:33 (17.8 MB/s) - ‘lotr.txt.1’ saved [147422]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/Fariaz99cuet/Character_level_GPT/blob/main/lotr.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HuhSv9YfhK61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "h4g-z0A6PV6Z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Config  Parameters\n"
      ],
      "metadata": {
        "id": "mr2IRbxYhMK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters=10000\n",
        "eval_iters=200\n",
        "learning_rate = 1e-3\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "eval_interval = 100"
      ],
      "metadata": {
        "id": "UrnPn2vcyvyV"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLb4_Xs5P5W2",
        "outputId": "7c84d4ef-1b7d-4be5-a377-26d63fc89013"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lord of the rings.txt\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "3CkxSS7SSi13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('lotr.txt.1','r', encoding ='utf-8') as f:\n",
        "\n",
        "  input =f.read()"
      ],
      "metadata": {
        "id": "hHXhna4GSo7g"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Length of dataset:\",len(input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sqkPF11Te-D",
        "outputId": "7671b814-2ff3-44ce-b864-80d4152b1066"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset: 2096842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#looking at first 500 characters\n",
        "print(input[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EgGiNZ1TmBf",
        "outputId": "ecaaccb4-b90d-41fd-85bb-0a5937831cfa"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "####-SPECIAL NOTE: \n",
            "\n",
            " \n",
            "In this reprint several minor inaccuracies, most of them noted by readers, have \n",
            "been corrected. For example, the rune text now corresponds exactly with the runes \n",
            "on  Thror's  Map.  More  important  is  the  matter  of  Chapter  Five.  There  the  true \n",
            "story  of  the  ending  of  the  Riddle  Game,  as  it  was  eventually  revealed  (under \n",
            "pressure)  by Bilbo  to Gandalf,  is  now  given  according  to  the Red Book,  in  place \n",
            "of  the  version  Bilbo  first  gave  to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vocab_size and vocabolary"
      ],
      "metadata": {
        "id": "AMeomJodhduu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(input)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQJvP0RdhHFA",
        "outputId": "548aed97-77bd-4b5d-bef6-68a0ec88ccd0"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"#&'()*,-./0123456789:;<=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ_`abcdefghijklmnopqrstuvwxyz}¢¥«®µ»ó–—‘’‚…\n",
            "98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gk1eKD5ZCSx",
        "outputId": "71d93154-a286-421a-ddd8-42f167b03c95"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Character Level simple Tokenization\n"
      ],
      "metadata": {
        "id": "tNywz2SRhkK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a character to integer mapping\n",
        "stoi = {ch:i for i,ch in enumerate(chars) }\n",
        "itos={i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda e : [stoi[c] for c in e]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n"
      ],
      "metadata": {
        "id": "B5aRZGxcofff"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode(\"When you play the game of thrones, you win or you die\"))\n",
        "print(decode(encode(\"When you play the game of thrones, you win or you die\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUp3gOlIp-IK",
        "outputId": "9e2f9bd4-08b9-4f7f-b18e-99f1ce8144a8"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[52, 65, 62, 71, 1, 82, 72, 78, 1, 73, 69, 58, 82, 1, 77, 65, 62, 1, 64, 58, 70, 62, 1, 72, 63, 1, 77, 65, 75, 72, 71, 62, 76, 10, 1, 82, 72, 78, 1, 80, 66, 71, 1, 72, 75, 1, 82, 72, 78, 1, 61, 66, 62]\n",
            "When you play the game of thrones, you win or you die\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding the entire text and storing it in torch tensor\n",
        "\n",
        "import torch\n",
        "data = torch.tensor(encode(input), dtype=torch.long)\n",
        "\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItrDI9Jqru55",
        "outputId": "cb6f4a2f-b7f5-40ee-9026-c6b02ff1cf7f"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2096842]) torch.int64\n",
            "tensor([ 4,  4,  4,  4, 11, 48, 45, 34, 32, 38, 30, 41,  1, 43, 44, 49, 34, 24,\n",
            "         1,  0,  0,  1,  0, 38, 71,  1, 77, 65, 66, 76,  1, 75, 62, 73, 75, 66,\n",
            "        71, 77,  1, 76, 62, 79, 62, 75, 58, 69,  1, 70, 66, 71, 72, 75,  1, 66,\n",
            "        71, 58, 60, 60, 78, 75, 58, 60, 66, 62, 76, 10,  1, 70, 72, 76, 77,  1,\n",
            "        72, 63,  1, 77, 65, 62, 70,  1, 71, 72, 77, 62, 61,  1, 59, 82,  1, 75,\n",
            "        62, 58, 61, 62, 75, 76, 10,  1, 65, 58, 79, 62,  1,  0, 59, 62, 62, 71,\n",
            "         1, 60, 72, 75, 75, 62, 60, 77, 62, 61, 12,  1, 35, 72, 75,  1, 62, 81,\n",
            "        58, 70, 73, 69, 62, 10,  1, 77, 65, 62,  1, 75, 78, 71, 62,  1, 77, 62,\n",
            "        81, 77,  1, 71, 72, 80,  1, 60, 72, 75, 75, 62, 76, 73, 72, 71, 61, 76,\n",
            "         1, 62, 81, 58, 60, 77, 69, 82,  1, 80, 66, 77, 65,  1, 77, 65, 62,  1,\n",
            "        75, 78, 71, 62, 76,  1,  0, 72, 71,  1,  1, 49, 65, 75, 72, 75,  6, 76,\n",
            "         1,  1, 42, 58, 73, 12,  1,  1, 42, 72, 75, 62,  1,  1, 66, 70, 73, 72,\n",
            "        75, 77, 58, 71, 77,  1,  1, 66, 76,  1,  1, 77, 65, 62,  1,  1, 70, 58,\n",
            "        77, 77, 62, 75,  1,  1, 72, 63,  1,  1, 32, 65, 58, 73, 77, 62, 75,  1,\n",
            "         1, 35, 66, 79, 62, 12,  1,  1, 49, 65, 62, 75, 62,  1,  1, 77, 65, 62,\n",
            "         1,  1, 77, 75, 78, 62,  1,  0, 76, 77, 72, 75, 82,  1,  1, 72, 63,  1,\n",
            "         1, 77, 65, 62,  1,  1, 62, 71, 61, 66, 71, 64,  1,  1, 72, 63,  1,  1,\n",
            "        77, 65, 62,  1,  1, 47, 66, 61, 61, 69, 62,  1,  1, 36, 58, 70, 62, 10,\n",
            "         1,  1, 58, 76,  1,  1, 66, 77,  1,  1, 80, 58, 76,  1,  1, 62, 79, 62,\n",
            "        71, 77, 78, 58, 69, 69, 82,  1,  1, 75, 62, 79, 62, 58, 69, 62, 61,  1,\n",
            "         1,  7, 78, 71, 61, 62, 75,  1,  0, 73, 75, 62, 76, 76, 78, 75, 62,  8,\n",
            "         1,  1, 59, 82,  1, 31, 66, 69, 59, 72,  1,  1, 77, 72,  1, 36, 58, 71,\n",
            "        61, 58, 69, 63, 10,  1,  1, 66, 76,  1,  1, 71, 72, 80,  1,  1, 64, 66,\n",
            "        79, 62, 71,  1,  1, 58, 60, 60, 72, 75, 61, 66, 71, 64,  1,  1, 77, 72,\n",
            "         1,  1, 77, 65, 62,  1, 47, 62, 61,  1, 31, 72, 72, 68, 10,  1,  1, 66,\n",
            "        71,  1,  1, 73, 69, 58, 60, 62,  1,  0, 72, 63,  1,  1, 77, 65, 62,  1,\n",
            "         1, 79, 62, 75, 76, 66, 72, 71,  1,  1, 31, 66, 69, 59, 72,  1,  1, 63,\n",
            "        66, 75, 76, 77,  1,  1, 64, 58, 79, 62,  1,  1, 77, 72,  1,  1, 65, 66,\n",
            "        76,  1,  1, 63, 75, 66, 62, 71, 61, 76, 10,  1,  1, 58, 71, 61,  1,  1,\n",
            "        58, 60, 77, 78, 58, 69, 69, 82,  1,  1, 76, 62, 77,  1,  1, 61, 72, 80,\n",
            "        71,  1,  1, 66, 71,  1,  1, 65, 66, 76,  1,  1, 61, 66, 58, 75, 82, 12,\n",
            "         1,  0, 49, 65, 66, 76,  1,  1, 61, 62, 73, 58, 75, 77, 78, 75, 62,  1,\n",
            "         1, 63, 75, 72, 70,  1,  1, 77, 75, 78, 77, 65,  1,  1, 72, 71,  1,  1,\n",
            "        77, 65, 62,  1,  1, 73, 58, 75, 77,  1,  1, 72, 63,  1,  1, 58,  1,  1,\n",
            "        70, 72, 76, 77,  1,  1, 65, 72, 71, 62, 76, 77,  1,  1, 65, 72, 59, 59,\n",
            "        66, 77,  1,  1, 80, 58, 76,  1,  1, 58,  1,  1, 73, 72, 75, 77, 62, 71,\n",
            "        77,  1,  1, 72, 63,  1,  0, 64, 75, 62, 58, 77,  1,  1, 76, 66, 64, 71,\n",
            "        66, 63, 66, 60, 58, 71, 60, 62, 12,  1,  1, 38, 77,  1,  1, 61, 72, 62,\n",
            "        76,  1,  1, 71, 72, 77, 10,  1,  1, 65, 72, 80, 62, 79, 62, 75, 10,  1,\n",
            "         1, 60, 72, 71, 60, 62, 75, 71,  1,  1, 77, 65, 62,  1,  1, 73, 75, 62,\n",
            "        76, 62, 71, 77,  1,  1, 76, 77, 72, 75, 82, 10,  1,  1, 58, 71, 61,  1,\n",
            "         1, 77, 65, 72, 76, 62,  1, 80, 65, 72,  1,  0, 66, 71,  1, 77, 65, 66,\n",
            "        76,  1, 62, 61, 66, 77, 66, 72, 71,  1, 70, 58, 68, 62,  1, 77, 65, 62,\n",
            "        66, 75,  1, 63, 66, 75, 76, 77,  1, 58, 60, 74, 78, 58, 66, 71, 77, 58,\n",
            "        71, 60, 62,  1, 80, 66, 77, 65,  1, 65, 72, 59, 59, 66, 77, 11, 69, 72,\n",
            "        75, 62,  1, 71, 62, 62, 61,  1, 71, 72, 77,  1, 77, 75, 72, 78, 73, 62,\n",
            "         1, 58, 59, 72, 78, 77,  1,  0, 66, 77, 12,  1,  1, 38, 77, 76,  1,  1,\n",
            "        62, 81, 73, 69, 58, 71, 58, 77, 66, 72, 71,  1,  1, 69, 66, 62, 76,  1,\n",
            "        66, 71,  1, 77, 65, 62,  1, 65, 66, 76, 77, 72, 75, 82,  1, 72, 63,  1,\n",
            "        77, 65, 62,  1, 47, 66, 71, 64, 10,  1, 58, 76,  1, 66, 77,  1, 80, 58,\n",
            "        76,  1, 76, 62, 77,  1, 72, 78, 77,  1, 66, 71,  1, 77, 65, 62,  1, 60,\n",
            "        65, 75, 72, 71, 66, 60, 69, 62, 76,  1,  0, 72, 63,  1, 77, 65, 62,  1,\n",
            "        47, 62, 61,  1, 31, 72, 72, 68,  1, 72, 63,  1, 52, 62, 76, 77, 70, 58,\n",
            "        75, 60, 65, 10,  1, 58, 71, 61,  1, 66, 76,  1, 71, 72, 80,  1, 77, 72,\n",
            "        69, 61,  1, 66, 71,  1, 49, 65, 62,  1, 41, 72, 75, 61,  1, 72, 63,  1,\n",
            "        77, 65, 62,  1, 47, 66, 71, 64, 76, 12])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "block_size =8\n",
        "batch_size=4\n",
        "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "ix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3OfhI0sy00T",
        "outputId": "7be346f9-8fa4-4358-e8b1-ec10fe8298c4"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([408484, 439577, 642178, 191201])"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " x = torch.stack([data[i:i+block_size] for i in ix])\n",
        " x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLyP32R9y_eW",
        "outputId": "9a1584fc-9dc9-43ed-e610-506ca5f9be24"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0, 31, 66, 69, 59, 72,  1,  1],\n",
              "        [65, 58, 69, 77,  1,  1, 58, 71],\n",
              "        [58, 64, 62,  1, 80, 65, 66, 60],\n",
              "        [ 1,  1, 51, 62, 75, 82,  1,  1]])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#spliting the data into train and validation set\n",
        "\n",
        "n =int(0.9*len(data))\n",
        "\n",
        "train = data[:n]\n",
        "\n",
        "val = data[n:]"
      ],
      "metadata": {
        "id": "DLcdsRG6qdhe"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz8KavGfszBJ",
        "outputId": "17e597b1-4d09-4122-8fa2-2b1c490617a1"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 4,  4,  4,  4, 11, 48, 45, 34, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x =train[:block_size]\n",
        "y = train[1:block_size+1]\n",
        "print(x,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bimXyrgVthMJ",
        "outputId": "12a89b62-a7f8-4ce4-cc10-7a851cf4412a"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 4,  4,  4,  4, 11, 48, 45, 34]) tensor([ 4,  4,  4, 11, 48, 45, 34, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting Batch Data"
      ],
      "metadata": {
        "id": "KQ_0xM9kh1QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "  data =train if split==\"train\" else val\n",
        "  ix=torch.randint(len(data)-block_size,(batch_size,))\n",
        "  x=torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y=torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x,y=x.to(device),y.to(device)\n",
        "  return x,y"
      ],
      "metadata": {
        "id": "d0p4_EBPwO9h"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Estimation"
      ],
      "metadata": {
        "id": "0b8aPxlDh8kI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_iters=200\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "VASBCGp0141_"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "n_embed=128"
      ],
      "metadata": {
        "id": "VsRa2gl-45X_"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single Headed Attention"
      ],
      "metadata": {
        "id": "EoWTVWGBiAzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "\n",
        "  \"\"\"One headed attention\"\"\"\n",
        "\n",
        "  def __init__(self,head_size):\n",
        "    super().__init__()\n",
        "    self.key=nn.Linear(n_embed,head_size,bias=False)\n",
        "    self.Query=nn.Linear(n_embed,head_size,bias=False)\n",
        "    self.Value=nn.Linear(n_embed,head_size,bias=False)\n",
        "    self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n",
        "\n",
        "    self.dropout =nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x) #(B,T,C)\n",
        "    q = self.Query(x) #(B,T,C)\n",
        "\n",
        "    wei= q @ k.transpose(-2,-1) ##(B,T,C) @ (B,C,T) = (B,T,T)\n",
        "    wei= wei.masked_fill(self.tril[:T,:T] ==0,float('-inf')) #(B,T,T)\n",
        "    wei =F.softmax(wei,dim=-1) #(B,T,T)\n",
        "    wei=self.dropout(wei)\n",
        "    v=self.Value(x)#(B,T,C)\n",
        "\n",
        "    out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z3qeafKu5J4M"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi Head Attention"
      ],
      "metadata": {
        "id": "6IdOPjMUiFmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  \"Multi headed attention \"\n",
        "  def __init__(self,num_head,head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_head)])\n",
        "    self.proj = nn.Linear(n_embed,n_embed)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    out = torch.cat([h(x) for h in self.heads],dim=-1)\n",
        "\n",
        "    out = self.dropout(self.proj(out))\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "9qhupA2WRxAI"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feed forward network"
      ],
      "metadata": {
        "id": "N3MSCbXWiKB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self,n_embed):\n",
        "    super().__init__()\n",
        "    self.net =nn.Sequential(\n",
        "        nn.Linear(n_embed,4*n_embed),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4*n_embed,n_embed),\n",
        "        nn.Dropout(dropout),)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4q_HOrGTUMNY"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single Transformer Block"
      ],
      "metadata": {
        "id": "ZXhNKJfriRRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size=n_embed//n_head\n",
        "    self.sa=MultiHeadAttention(n_head,head_size)\n",
        "    self.ffwd=FeedForward(n_embed)\n",
        "    self.ln1=nn.LayerNorm(n_embed)\n",
        "    self.ln2=nn.LayerNorm(n_embed)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "h-1TGOY_eLlz"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "viG536AYpSoB"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bigram Model for Text Generation"
      ],
      "metadata": {
        "id": "UcADRY60iXas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Bigram model\n",
        "\n",
        "n_layers =16\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "     super().__init__()\n",
        "     #each token reads logits of next token from embedding lookup table\n",
        "\n",
        "     self.token_embedding_table = nn.Embedding(vocab_size,n_embed)\n",
        "     self.position_embedding_table=nn.Embedding(block_size,n_embed)\n",
        "     self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
        "     self.ln=nn.LayerNorm(n_embed)\n",
        "     self.ln_head=nn.Linear(n_embed,vocab_size)\n",
        "\n",
        "  def forward(self,idx,target=None):\n",
        "    B,T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T,device=device))\n",
        "    x = tok_emb + pos_emb #(B,T,c)\n",
        "    x = self.blocks(x)#(B,T,c)\n",
        "    x = self.ln(x)#(B,T,c)\n",
        "    logits = self.ln_head(x) #(B,T,vocab_size)\n",
        "\n",
        "    if target is None:\n",
        "      loss=None\n",
        "    else :\n",
        "      B, T, C = logits.shape\n",
        "\n",
        "      logits = logits.view(B*T,C)\n",
        "      target = target.view(B*T)\n",
        "\n",
        "      loss=F.cross_entropy(logits,target)\n",
        "\n",
        "    return logits,loss\n",
        "\n",
        "\n",
        "  def generate(self,idx,max_new_tokens):\n",
        "\n",
        "\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "      idx_cond = idx[:,-block_size:]\n",
        "\n",
        "      logits,loss=self(idx_cond)\n",
        "\n",
        "      logits = logits[:,-1,:] #(B,C)\n",
        "\n",
        "      prob = F.softmax(logits,dim=-1)\n",
        "\n",
        "      idx_next = torch.multinomial(prob,num_samples=1)\n",
        "\n",
        "      idx=torch.cat((idx,idx_next),dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8dtwyN_ggWDh"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model"
      ],
      "metadata": {
        "id": "5XaDhOEAidHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = BigramLanguageModel()\n",
        "\n",
        "m=model.to(device)\n",
        "\n",
        "print(sum(p.numel() for p in model.parameters())/1e6,'M Parameters')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_interval==0 or iter==eval_interval-1:\n",
        "\n",
        "    losses=estimate_loss()\n",
        "    print(f\"step {iter} : train_loss={losses['train']:.4f} val_loss={losses['val']:.4f}\")\n",
        "\n",
        "    x_batch,y_batch=get_batch('train')\n",
        "\n",
        "    logits,loss=model(x_batch,y_batch)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=None)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CkXXHAHrvjr",
        "outputId": "00df705f-d441-48f2-a215-0ec8b530ae1f"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.82109 M Parameters\n",
            "step 0 : train_loss=4.6874 val_loss=4.6867\n",
            "step 99 : train_loss=4.0561 val_loss=4.0812\n",
            "step 100 : train_loss=3.7114 val_loss=3.7491\n",
            "step 200 : train_loss=3.5123 val_loss=3.5470\n",
            "step 300 : train_loss=3.3696 val_loss=3.4070\n",
            "step 400 : train_loss=3.2674 val_loss=3.3027\n",
            "step 500 : train_loss=3.1953 val_loss=3.2234\n",
            "step 600 : train_loss=3.1367 val_loss=3.1730\n",
            "step 700 : train_loss=3.0875 val_loss=3.1210\n",
            "step 800 : train_loss=3.0420 val_loss=3.0792\n",
            "step 900 : train_loss=3.0083 val_loss=3.0467\n",
            "step 1000 : train_loss=2.9861 val_loss=3.0162\n",
            "step 1100 : train_loss=2.9622 val_loss=2.9943\n",
            "step 1200 : train_loss=2.9423 val_loss=2.9772\n",
            "step 1300 : train_loss=2.9205 val_loss=2.9611\n",
            "step 1400 : train_loss=2.9034 val_loss=2.9355\n",
            "step 1500 : train_loss=2.8888 val_loss=2.9222\n",
            "step 1600 : train_loss=2.8593 val_loss=2.8971\n",
            "step 1700 : train_loss=2.8508 val_loss=2.8814\n",
            "step 1800 : train_loss=2.8273 val_loss=2.8645\n",
            "step 1900 : train_loss=2.8126 val_loss=2.8377\n",
            "step 2000 : train_loss=2.7959 val_loss=2.8227\n",
            "step 2100 : train_loss=2.7765 val_loss=2.8048\n",
            "step 2200 : train_loss=2.7593 val_loss=2.7860\n",
            "step 2300 : train_loss=2.7417 val_loss=2.7763\n",
            "step 2400 : train_loss=2.7310 val_loss=2.7720\n",
            "step 2500 : train_loss=2.7167 val_loss=2.7534\n",
            "step 2600 : train_loss=2.7150 val_loss=2.7476\n",
            "step 2700 : train_loss=2.7118 val_loss=2.7379\n",
            "step 2800 : train_loss=2.6894 val_loss=2.7220\n",
            "step 2900 : train_loss=2.6759 val_loss=2.7146\n",
            "step 3000 : train_loss=2.6715 val_loss=2.7065\n",
            "step 3100 : train_loss=2.6680 val_loss=2.6929\n",
            "step 3200 : train_loss=2.6532 val_loss=2.6938\n",
            "step 3300 : train_loss=2.6593 val_loss=2.6776\n",
            "step 3400 : train_loss=2.6555 val_loss=2.6742\n",
            "step 3500 : train_loss=2.6376 val_loss=2.6632\n",
            "step 3600 : train_loss=2.6209 val_loss=2.6613\n",
            "step 3700 : train_loss=2.6223 val_loss=2.6541\n",
            "step 3800 : train_loss=2.6221 val_loss=2.6385\n",
            "step 3900 : train_loss=2.6156 val_loss=2.6422\n",
            "step 4000 : train_loss=2.6048 val_loss=2.6417\n",
            "step 4100 : train_loss=2.6038 val_loss=2.6232\n",
            "step 4200 : train_loss=2.5924 val_loss=2.6226\n",
            "step 4300 : train_loss=2.5920 val_loss=2.6173\n",
            "step 4400 : train_loss=2.5849 val_loss=2.6097\n",
            "step 4500 : train_loss=2.5706 val_loss=2.6048\n",
            "step 4600 : train_loss=2.5663 val_loss=2.6064\n",
            "step 4700 : train_loss=2.5877 val_loss=2.6086\n",
            "step 4800 : train_loss=2.5742 val_loss=2.5979\n",
            "step 4900 : train_loss=2.5739 val_loss=2.5966\n",
            "step 5000 : train_loss=2.5733 val_loss=2.6000\n",
            "step 5100 : train_loss=2.5705 val_loss=2.5904\n",
            "step 5200 : train_loss=2.5616 val_loss=2.5949\n",
            "step 5300 : train_loss=2.5557 val_loss=2.5908\n",
            "step 5400 : train_loss=2.5565 val_loss=2.5801\n",
            "step 5500 : train_loss=2.5539 val_loss=2.5796\n",
            "step 5600 : train_loss=2.5555 val_loss=2.5757\n",
            "step 5700 : train_loss=2.5528 val_loss=2.5693\n",
            "step 5800 : train_loss=2.5515 val_loss=2.5773\n",
            "step 5900 : train_loss=2.5443 val_loss=2.5668\n",
            "step 6000 : train_loss=2.5443 val_loss=2.5718\n",
            "step 6100 : train_loss=2.5391 val_loss=2.5655\n",
            "step 6200 : train_loss=2.5228 val_loss=2.5665\n",
            "step 6300 : train_loss=2.5301 val_loss=2.5530\n",
            "step 6400 : train_loss=2.5233 val_loss=2.5542\n",
            "step 6500 : train_loss=2.5266 val_loss=2.5476\n",
            "step 6600 : train_loss=2.5229 val_loss=2.5460\n",
            "step 6700 : train_loss=2.5272 val_loss=2.5370\n",
            "step 6800 : train_loss=2.5179 val_loss=2.5435\n",
            "step 6900 : train_loss=2.5073 val_loss=2.5356\n",
            "step 7000 : train_loss=2.5023 val_loss=2.5347\n",
            "step 7100 : train_loss=2.5054 val_loss=2.5330\n",
            "step 7200 : train_loss=2.5008 val_loss=2.5340\n",
            "step 7300 : train_loss=2.5150 val_loss=2.5388\n",
            "step 7400 : train_loss=2.5186 val_loss=2.5471\n",
            "step 7500 : train_loss=2.5228 val_loss=2.5393\n",
            "step 7600 : train_loss=2.5137 val_loss=2.5369\n",
            "step 7700 : train_loss=2.5205 val_loss=2.5246\n",
            "step 7800 : train_loss=2.5046 val_loss=2.5191\n",
            "step 7900 : train_loss=2.5019 val_loss=2.5211\n",
            "step 8000 : train_loss=2.4990 val_loss=2.5184\n",
            "step 8100 : train_loss=2.4931 val_loss=2.5157\n",
            "step 8200 : train_loss=2.4943 val_loss=2.5169\n",
            "step 8300 : train_loss=2.4919 val_loss=2.5112\n",
            "step 8400 : train_loss=2.4945 val_loss=2.5137\n",
            "step 8500 : train_loss=2.4976 val_loss=2.5180\n",
            "step 8600 : train_loss=2.4946 val_loss=2.5206\n",
            "step 8700 : train_loss=2.4867 val_loss=2.5069\n",
            "step 8800 : train_loss=2.4772 val_loss=2.5066\n",
            "step 8900 : train_loss=2.4790 val_loss=2.4990\n",
            "step 9000 : train_loss=2.4747 val_loss=2.4876\n",
            "step 9100 : train_loss=2.4688 val_loss=2.4909\n",
            "step 9200 : train_loss=2.4768 val_loss=2.4890\n",
            "step 9300 : train_loss=2.4685 val_loss=2.4945\n",
            "step 9400 : train_loss=2.4616 val_loss=2.4797\n",
            "step 9500 : train_loss=2.4696 val_loss=2.4822\n",
            "step 9600 : train_loss=2.4570 val_loss=2.4810\n",
            "step 9700 : train_loss=2.4577 val_loss=2.4890\n",
            "step 9800 : train_loss=2.4630 val_loss=2.4838\n",
            "step 9900 : train_loss=2.4508 val_loss=2.4769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generation Of text"
      ],
      "metadata": {
        "id": "DpFM5DZ0ig78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=1500)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiJs9gEC4sm6",
        "outputId": "e63ccfb5-c0d6-4b37-8487-ab897f5deeed"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Wes he amecey Oreyhund igy, cad frencl#n trinan  Goun s amen weds-sthes yre   an tive as TboumAve. sthamol thed  t  wanoud  fs  toomamperer thinenthind tro spored, mashaounasong  Bbipe anAd mee6 \n",
            "s \n",
            " t  s \n",
            " bltoktwee\"oukas  in cowiloultro aGaved looeine imey las' Rokevt Gm. ace  cin  nyomut t as  acolingshee he  0s Yevhingeres  s w\n",
            "uind jom  tee in  rr. I'Erors anitond  \"us C thilly owe pebed ge leecEne.   `  havorine d e iced  wacles \n",
            " tey ing thsheculain   mer t thte-oekeat, ind. lik thestaRote\" ds toainey d   wind rithted pe t  theupid. und  gurtipee E cougang \n",
            "  wero Eyst, tomesoupobilas hid   yelrey toullilik in cupheca>e   inevetachanou.nenin sntot o wane,  \n",
            "b ! \n",
            "uy id, bind 'arim, in, cod fy  then,es &o! aknenitiysby ce in, htupais totostre inareuthe \n",
            "s.  Rblkerousnle \n",
            ": \n",
            "ghe. -  delfindelis myowan,' imroond    lorou ad oed amle \n",
            "2or c1veed  ren  \n",
            "s-/atlalllogband \n",
            "ini.deed Ysitgof t waninoug \n",
            "\n",
            ": n  fif \n",
            "vendet t ce Ce in o  intonthe? louldoulrmy Aplldilband IRinc, oameres as baun! to' tibLine  \n",
            " wwnassiymithincinderyz as  moSond  wulone sbiind  ;veitudeNout itit y,  t, ttend aatleid.   ililheycey, rnivetu-lo be m   yeil \n",
            "  \n",
            " unt *lon `eKinlirr  Ro an  \n",
            "  sey,  k wt mestoit r. wanlled 't andpis  'sed herp \n",
            "onendenol t, s piwe \n",
            "the coithil pe sCin then an, net o  lllcome coudon '' tad fanos th ' cein  it a S   ' mpancowe therthain  iro co  frel \n",
            " ts lld  bymindxak b thalomere iny t goullsstoth henathtPistein t s ceere sth  iluthenganng ug,  ted Ishis \n",
            "nd as #hirrlid, in\n"
          ]
        }
      ]
    }
  ]
}